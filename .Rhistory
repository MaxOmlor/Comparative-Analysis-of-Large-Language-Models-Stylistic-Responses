library(topicmodels)
# text encoding funcs------------------------
encode_none <- function(text) {
return(text)
}
encode_sentence_len <- function(text) {
# Zerlege den Text in Sätze unter Berücksichtigung von .!? gefolgt von Leerzeichen oder Zeilenumbrüchen
# sentences <- unlist(str_split(text, "(?<=[.!?])([\\s]|\\r?\\n)+"))
sentences <- unlist(str_split(text, "((?<=[.!?])([\\s]|\\r?\\n)+)|(?=[\\r\\n])"))
# print(sentences)
clean_sentences <- unname(sapply(sentences, function(sentence) {
# Satzzeichen, Zeilenumbrüche und Tabulatoren entfernen
clean_sentence <- gsub("[[:punct:]]", " ", sentence)
clean_sentence <- gsub("\\n", " ", clean_sentence)
clean_sentence <- gsub("\\t", " ", clean_sentence)
# Entferne zusätzliche Leerzeichen, die durch die Ersetzung entstanden sein könnten
clean_sentence <- gsub("\\s+", " ", clean_sentence)
clean_sentence <- trimws(clean_sentence)
return(clean_sentence)
}))
# print(clean_sentences)
# Kodiere jeden Satz durch ein Wort basierend auf seiner Wortanzahl
encoded_sentences <- unname(sapply(clean_sentences, function(sentence) {
words <- str_split(sentence, "\\s+")[[1]] # Zerlege jeden Satz in Wörter
word_count <- length(words) # Zähle die Anzahl der Wörter
# return(paste0("len", word_count)) # Erstelle das kodierende Wort basierend auf der Wortanzahl
return(strrep("x", word_count))
}))
# print(encoded_sentences)
# Kombiniere die kodierten Wörter zu einem Text
encoded_text <- paste(encoded_sentences, collapse=" ")
return(encoded_text)
}
encode_punctuation <- function(text) {
# Schritt 1: Entferne alle Zeichen, die nicht .,!?; sind
text <- gsub("[^.,!?;]", "", text)
# Schritt 2: Dieser Schritt ist technisch nicht nötig, da alle nicht Satzzeichen bereits entfernt wurden
# Schritt 3: Ersetze jedes Satzzeichen durch das entsprechende Wort
text <- gsub("\\.", " PERIOD ", text)
text <- gsub(",", " COMMA ", text)
text <- gsub("\\?", " QUESTIONMARK ", text)
text <- gsub("!", " EXCLAMATIONMARK ", text)
text <- gsub(";", " SEMICOLON ", text)
# Entferne zusätzliche Leerzeichen, die durch die Ersetzung entstanden sein könnten
text <- gsub("\\s+", " ", text)
text <- trimws(text) # Entferne Leerzeichen am Anfang und Ende des Strings
return(text)
}
encode_sentence_len_punctuation <- function(text) {
sentence_len <- encode_sentence_len(text)
punctuation <- encode_punctuation(text)
combined_text <- paste(sentence_len, punctuation, sep = "\n")
return(combined_text)
}
encode_topic_modelling <- function(text, k = 5, word_count = 20) {
# Vorbereitung des Texts
corpus <- Corpus(VectorSource(text))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
# Erstellung einer Document-Term-Matrix
dtm <- DocumentTermMatrix(corpus)
# Durchführung des Topic-Modellings
lda_model <- LDA(dtm, k = k)
# Extraktion der wichtigsten Themen
topics <- terms(lda_model, word_count)
# Erstellen eines Bag-of-Words
words <- unlist(topics)
# Umwandeln der Liste von Wörtern in einen String
word_string <- paste(words, collapse = " ")
return(word_string)
}
encode_topic_words <- function(text) {
# Umwandlung in Kleinbuchstaben
text <- tolower(text)
# Entfernung von Satzzeichen
text <- gsub("[[:punct:]]", " ", text)
# Entfernung von Zahlen
text <- gsub("[[:digit:]]", " ", text)
# Entfernung von Stoppwörtern
text <- removeWords(text, stopwords("english"))
# Entfernung von überflüssigen Leerzeichen
text <- gsub("\\s+", " ", text)
return(text)
}
# Dictionary der Kodierungsfunktionen
encoding_functions <- list(
"none" = encode_none,
"sentence_len" = encode_sentence_len,
"punctuation" = encode_punctuation,
"sentence_len_punctuation" = encode_sentence_len_punctuation,
"topic_modelling" = encode_topic_modelling,
"topic_words" = encode_topic_words
)
# Funktion zur Auswahl der Encoding-Funktion basierend auf dem Dictionary
select_encoding_func <- function(config) {
if ("encoding" %in% names(config)) {
encoding <- config$encoding
} else {
cat("Encode field not found in config. Setting encoding to 'none'.\n")
encoding <- "none"
}
cat(encoding, "selected.\n")
if (encoding %in% names(encoding_functions)) {
return(encoding_functions[[encoding]])
} else {
stop("Invalid encoding option in config file.")
}
}
remove_comments <- function(text) {
# Zeilen des Textes in ein Vektor aufteilen
lines <- unlist(strsplit(text, "\n"))
# Entferne Zeilen, die mit "#" am Anfang beginnen
cleaned_lines <- lines[!grepl("^#", lines)]
# Modifizierten Text zusammenführen
cleaned_text <- paste(cleaned_lines, collapse = "\n")
return(cleaned_text)
}
construct_corpus <- function(json_path) {
cat("json_path:", json_path, "\n")
# JSON-Datei einlesen
config <- fromJSON(json_path)
# print(config)
# Output-Verzeichnis aus dem config-Objekt
output_dir <- config$output_dir
# print(output_dir)
if (dir.exists(output_dir)) {
return()
}
# clear output_dir from old files
unlink(output_dir, recursive = TRUE)
# Verzeichnis erstellen, falls es nicht existiert
dir_create(output_dir)
# Encoding-Funktion auswählen
encoding_func <- select_encoding_func(config)
process_sources <- function(input_dir, label, sublabel) {
# cat("Input Dir:", input_dir, "\nLabel:", label, "\nSublabel:", sublabel, "\n")
# Liste aller .txt-Dateien im input_dir
files <- dir_ls(path = input_dir, regexp = "\\.txt$")
# print(length(files))
# Dateien durchlaufen und kopieren/umbenennen
for (i in seq_along(files)) {
file_path <- files[i]
new_name <- sprintf("%s_%s%02d.txt", label, sublabel, i)
# print(file.path(output_dir, new_name))
# Text einlesen
text <- readLines(file_path, encoding = "UTF-8")
# remove comments
clean_text <- remove_comments(text)
# Text mit der ausgewählten Encoding-Funktion kodieren
encoded_text <- encoding_func(clean_text)
# Kodierten Text im Ausgabeverzeichnis abspeichern
writeLines(encoded_text, file.path(output_dir, new_name), useBytes = TRUE)
}
}
# copy and rename files
mapply(FUN=process_sources, input_dir=config$sources$input_dir, label=config$sources$label, sublabel=config$sources$sublabel)
}
# config_dir <- "C:/Users/Max/Documents/Studium/Leipzig/M 5. Semester/dh/data/corpus_configs"
config_dir <- "../data/corpus_configs"
config_names <- list.files(config_dir)
config_paths <- file.path(config_dir, config_names)
print(config_paths)
# config_dir <- "C:/Users/Max/Documents/Studium/Leipzig/M 5. Semester/dh/data/corpus_configs"
config_dir <- "data/corpus_configs"
config_names <- list.files(config_dir)
config_paths <- file.path(config_dir, config_names)
print(config_paths)
mapply(FUN=construct_corpus, json_path=config_paths)
if (!require("stringr")) install.packages("stringr")
if (!require("plotly")) install.packages("plotly")
if (!require("webshot")) install.packages("webshot")
library(stringr)
library(purrr)
library(plotly)
library(webshot)
library(ggplot2)
load_txt_files <- function(dir_path) {
# Liste aller .txt-Dateien im Verzeichnis erhalten
txt_files <- list.files(dir_path, pattern = "\\.txt$", full.names = TRUE)
# Text aus allen .txt-Dateien laden
all_text <- lapply(txt_files, readLines)
return(all_text)
}
remove_comments <- function(text) {
# Zeilen des Textes in ein Vektor aufteilen
lines <- unlist(strsplit(text, "\n"))
# Entferne Zeilen, die mit "#" am Anfang beginnen
cleaned_lines <- lines[!grepl("^#", lines)]
# Modifizierten Text zusammenführen
cleaned_text <- paste(cleaned_lines, collapse = "\n")
return(cleaned_text)
}
get_clean_text <- function(text) {
# Satzzeichen, Zeilenumbrüche und Tabulatoren entfernen
clean_text <- gsub("[[:punct:]]", " ", text)
clean_text <- gsub("\\n", " ", clean_text)
clean_text <- gsub("\\t", " ", clean_text)
# Entferne zusätzliche Leerzeichen, die durch die Ersetzung entstanden sein könnten
clean_text <- gsub("\\s+", " ", clean_text)
clean_text <- trimws(clean_text)
return(clean_text)
}
count_words <- function(text) {
clean_text <- get_clean_text(text)
# print(clean_text)
# Text in Wörter aufteilen
words <- strsplit(clean_text, "\\s+")[[1]]
# print(words)
# Anzahl der Wörter zählen
word_count <- length(words)
return(word_count)
}
agg_word_counts <- function(dir_path) {
text_list <- load_txt_files(dir_path)
text_list <- sapply(text_list, remove_comments)
# print(text_list)
word_counts <- unname(sapply(text_list, count_words))
#print(word_counts)
return(word_counts)
}
agg_dates <- function(dir_path) {
txt_files <- list.files(dir_path, pattern = "\\.txt$", full.names = TRUE)
txt_files <- unname(sapply(txt_files, basename))
prefixes <- str_extract_all(txt_files, "^\\d{4}", simplify = TRUE)
# prefixes_int <- as.integer(prefixes)
prefixes_int <- unname(sapply(prefixes, as.integer))
return(prefixes_int)
}
hist_agg_values <- function(values, breaks) {
# Berechne die Anzahl der breaks
num_breaks <- length(breaks) - 1
# Initialisiere das Dictionary für die aggregierten Werte
aggregated <- list()
# Durchlaufe jeden break
for (i in seq_len(num_breaks)) {
# Zähle die Anzahl der Werte, die in diesem break liegen
count <- sum(values >= breaks[i] & values < breaks[i + 1])
# Füge den aggregierten Wert dem Dictionary hinzu
aggregated[[paste0(breaks[i], "-", breaks[i + 1])]] <- count
}
return(aggregated)
}
multi_hist <- function(datas, main_title, legend, breaks) {
lists <- unname(lapply(datas, partial(hist_agg_values, breaks=breaks)))
# Automatische Farbwahl für die Säulen
num_lists <- length(lists)
colors <- rainbow(num_lists)
# Extrahiere alle eindeutigen keys
all_keys <- unique(unlist(lapply(lists, names)))
# Plot erstellen
barplot_matrix <- matrix(nrow = length(all_keys), ncol = num_lists)
for (i in seq_along(lists)) {
list <- lists[[i]]
for (j in seq_along(all_keys)) {
key <- all_keys[j]
if (key %in% names(list)) {
value <- list[[key]]
barplot_matrix[j, i] <- value
} else {
barplot_matrix[j, i] <- 0
}
}
}
# Plot erstellen
barplot(
barplot_matrix,
beside = TRUE,
main = main_title,
col = colors,
width = num_lists * length(breaks)
)
# x-Achse mit benutzerdefinierten Einheiten beschriften
axis(1, at = 1:length(breaks), labels = breaks)
# Legende hinzufügen
legend("topright", legend = legend, fill = colors)
}
multi_hist <- function(datas, main_title, legend, breaks) {
# Automatische Farbwahl
num_datas <- length(datas)
colors <- rainbow(num_datas)
# Berechnung des Minimums und Maximums über alle Daten
#min_val <- min(unlist(datas))
#max_val <- max(unlist(datas))
min_val <- min(unlist(breaks))
max_val <- max(unlist(breaks))
# Histogramme erstellen
# par(mar = c(5, 5, 2, 2)) # Setze die Grafikparameter für die Margen
for (i in seq_along(datas)) {
if (i == 0) {
hist(
datas[[i]],
col = colors[i],
xlim = c(min_val, max_val),
main = main_title,
breaks = breaks
)
} else {
hist(
datas[[i]],
col = colors[i],
add=TRUE
)
}
}
# Legende hinzufügen
legend("topright", legend = legend, fill = colors)
}
gen_agg_hists <- function(dir_paths, agg_func, agg_title, breaks, img_output_dir) {
dir_name <- basename(dir_path)
out_file_path <- file.path(img_output_dir, paste0("hist", agg_title, ".jpeg"))
legend = lapply(dir_paths, basename)
agg_vecs <- lapply(dir_paths, agg_func)
print(agg_vecs)
fig <- plot_ly()
for (i in seq_along(agg_vecs)){
fig <- fig %>%
add_histogram(x = agg_vecs[[i]], name = legend[i])
}
fig <- fig %>%
layout(title = paste("Histogram over", agg_title),
xaxis = list(title = agg_title),
yaxis = list(title = "Frequency"),
barmode = "group")
# Speichern der Figur als JPG
# orca(fig, file = out_file_path, width = 800, height = 600)
export(fig, file = out_file_path)
# Render the plot
print(fig)
}
#dir_paths <- c(
#  "data/politicians/John F. Kennedy",
#  "data/politicians/Margaret Thatcher",
#  "data/politicians/Winston Churchill",
#  "data/climate change/greta thunberg",
#  "data/climate change/un",
#  "data/climate change/unfccc"
#)
dir_paths <- c(
"data/gpt3_5/climate change",
"data/gpt3_5/climate change John F. Kennedy",
"data/gpt3_5/climate change Margaret Thatcher",
"data/gpt3_5/climate change winston churchill",
"data/gpt3_5/John F. Kennedy",
"data/gpt3_5/Margaret Thatcher",
"data/gpt3_5/winston churchill",
"data/gpt3_5/rag winston churchill"
)
gen_agg_hists(
dir_paths = dir_paths,
agg_func = agg_word_counts,
agg_title = "word counts",
breaks = seq(0, 8000, by = 100),
img_output_dir
)
gen_agg_hists(
dir_paths = dir_paths,
agg_func = agg_dates,
agg_title = "date",
breaks = seq(1900, 2030, by = 10),
img_output_dir
)
gen_agg_hists(
dir_paths = dir_paths,
agg_func = agg_word_counts,
agg_title = "word counts",
breaks = seq(0, 8000, by = 100),
img_output_dir
)
gen_agg_hists <- function(dir_paths, agg_func, agg_title, breaks, img_output_dir) {
# dir_name <- basename(dir_path)
out_file_path <- file.path(img_output_dir, paste0("hist", agg_title, ".jpeg"))
legend = lapply(dir_paths, basename)
agg_vecs <- lapply(dir_paths, agg_func)
print(agg_vecs)
fig <- plot_ly()
for (i in seq_along(agg_vecs)){
fig <- fig %>%
add_histogram(x = agg_vecs[[i]], name = legend[i])
}
fig <- fig %>%
layout(title = paste("Histogram over", agg_title),
xaxis = list(title = agg_title),
yaxis = list(title = "Frequency"),
barmode = "group")
# Speichern der Figur als JPG
# orca(fig, file = out_file_path, width = 800, height = 600)
export(fig, file = out_file_path)
# Render the plot
print(fig)
}
#dir_paths <- c(
#  "data/politicians/John F. Kennedy",
#  "data/politicians/Margaret Thatcher",
#  "data/politicians/Winston Churchill",
#  "data/climate change/greta thunberg",
#  "data/climate change/un",
#  "data/climate change/unfccc"
#)
dir_paths <- c(
"data/gpt3_5/climate change",
"data/gpt3_5/climate change John F. Kennedy",
"data/gpt3_5/climate change Margaret Thatcher",
"data/gpt3_5/climate change winston churchill",
"data/gpt3_5/John F. Kennedy",
"data/gpt3_5/Margaret Thatcher",
"data/gpt3_5/winston churchill",
"data/gpt3_5/rag winston churchill"
)
gen_agg_hists(
dir_paths = dir_paths,
agg_func = agg_word_counts,
agg_title = "word counts",
breaks = seq(0, 8000, by = 100),
img_output_dir
)
gen_agg_hists <- function(dir_paths, agg_func, agg_title, breaks, img_output_dir) {
# dir_name <- basename(dir_path)
# out_file_path <- file.path(img_output_dir, paste0("hist", agg_title, ".jpeg"))
legend = lapply(dir_paths, basename)
agg_vecs <- lapply(dir_paths, agg_func)
print(agg_vecs)
fig <- plot_ly()
for (i in seq_along(agg_vecs)){
fig <- fig %>%
add_histogram(x = agg_vecs[[i]], name = legend[i])
}
fig <- fig %>%
layout(title = paste("Histogram over", agg_title),
xaxis = list(title = agg_title),
yaxis = list(title = "Frequency"),
barmode = "group")
# Speichern der Figur als JPG
# orca(fig, file = out_file_path, width = 800, height = 600)
export(fig, file = out_file_path)
# Render the plot
print(fig)
}
#dir_paths <- c(
#  "data/politicians/John F. Kennedy",
#  "data/politicians/Margaret Thatcher",
#  "data/politicians/Winston Churchill",
#  "data/climate change/greta thunberg",
#  "data/climate change/un",
#  "data/climate change/unfccc"
#)
dir_paths <- c(
"data/gpt3_5/climate change",
"data/gpt3_5/climate change John F. Kennedy",
"data/gpt3_5/climate change Margaret Thatcher",
"data/gpt3_5/climate change winston churchill",
"data/gpt3_5/John F. Kennedy",
"data/gpt3_5/Margaret Thatcher",
"data/gpt3_5/winston churchill",
"data/gpt3_5/rag winston churchill"
)
gen_agg_hists(
dir_paths = dir_paths,
agg_func = agg_word_counts,
agg_title = "word counts",
breaks = seq(0, 8000, by = 100),
img_output_dir
)
gen_agg_hists <- function(dir_paths, agg_func, agg_title, breaks, img_output_dir) {
# dir_name <- basename(dir_path)
# out_file_path <- file.path(img_output_dir, paste0("hist", agg_title, ".jpeg"))
legend = lapply(dir_paths, basename)
agg_vecs <- lapply(dir_paths, agg_func)
print(agg_vecs)
fig <- plot_ly()
for (i in seq_along(agg_vecs)){
fig <- fig %>%
add_histogram(x = agg_vecs[[i]], name = legend[i])
}
fig <- fig %>%
layout(title = paste("Histogram over", agg_title),
xaxis = list(title = agg_title),
yaxis = list(title = "Frequency"),
barmode = "group")
# Speichern der Figur als JPG
# orca(fig, file = out_file_path, width = 800, height = 600)
# export(fig, file = out_file_path)
# Render the plot
print(fig)
}
#dir_paths <- c(
#  "data/politicians/John F. Kennedy",
#  "data/politicians/Margaret Thatcher",
#  "data/politicians/Winston Churchill",
#  "data/climate change/greta thunberg",
#  "data/climate change/un",
#  "data/climate change/unfccc"
#)
dir_paths <- c(
"data/gpt3_5/climate change",
"data/gpt3_5/climate change John F. Kennedy",
"data/gpt3_5/climate change Margaret Thatcher",
"data/gpt3_5/climate change winston churchill",
"data/gpt3_5/John F. Kennedy",
"data/gpt3_5/Margaret Thatcher",
"data/gpt3_5/winston churchill",
"data/gpt3_5/rag winston churchill"
)
gen_agg_hists(
dir_paths = dir_paths,
agg_func = agg_word_counts,
agg_title = "word counts",
breaks = seq(0, 8000, by = 100),
img_output_dir
)
